[llm-server]
# https://github.com/ggerganov/llama.cpp/tree/master/examples/server
threads = 128
threads-batch = 128
model = "./models/mixtral8x7b-q4/mixtral-8x7b-v0.1.Q4_K_M.gguf"
alias = "NITMRE-LLM"
ctx-size = 4096
n-gpu-layers = 128
main-gpu = 0
# tensor-split = []
batch-size = 512
# memory-f32
mlock = false
# no-mmap
# numa
# lora
# lora-base
timeout = 600
host = "http://127.0.0.1"
port = 8080
path = "examples/server/public"
# api-key
# api-key-file
# embedding
parallel = 8
cont-batching = false
# system-prompt-file
# mmproj

[api-server]
host = "127.0.0.1"
port = 8000

[mixtral8x7b]
name="mixtral8x7b"
stream = true
n_predict = 400
temperature = 0.7
stop = ["</s>", "mixtral8x7b:", "user:"]
repeat_last_n = 256
repeat_penalty = 1.18
top_k = 40
top_p = 0.95
min_p = 0.05
tfs_z = 1
typical_p = 1
presence_penalty = 0
frequency_penalty = 0
mirostat = 0
mirostat_tau = 5
mirostat_eta = 0.1
grammar = ""
n_probs = 0
image_data = []
cache_prompt = false
api_key = ""
slot_id = 0
